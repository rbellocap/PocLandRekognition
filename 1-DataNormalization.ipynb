{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation des données\n",
    "\n",
    "Afin de permettre le traitement des données sur le cloud Amazon, il nous faut normaliser nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "%pip install boto3\n",
    "%pip install smart_open\n",
    "%pip install python-dotenv\n",
    "# https://github.com/GeospatialPython/pyshp\n",
    "%pip install pyshp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connexion au bucket S3 Amazon\n",
    "\n",
    "Pour pouvoir travailler avec nos données, nous utilisons un bucket S3 contenant les dataset. Afin de permettre la connexion au service S3, il est nécessaire d'avoir défini un fichier `.env` contenant les variables d'environnements suivantes :\n",
    "\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=xxxxxx\n",
    "AWS_SECRET_ACCESS_KEY=xxxxxx\n",
    "```\n",
    "\n",
    "Ensuite, il suffit d'ouvrir une session à l'aide de la librairie `boto3` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of test.txt file: HELLO WORLD !\n"
     ]
    }
   ],
   "source": [
    "import os, boto3, dotenv\n",
    "from smart_open import open\n",
    "\n",
    "dotenv.load_dotenv('./.env', override=True)\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    ")\n",
    "\n",
    "url = 's3://poc-automatic-land-parcel-recognition/test.txt'\n",
    "\n",
    "with open(url, 'rb', transport_params = {'client': session.client('s3')}) as reader:\n",
    "    for line in reader:\n",
    "        print(line.decode('utf8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fichiers Shapefile\n",
    "\n",
    "L'étape suivante est d'apprendre à traiter les données parcellaires à notre disposition contenues dans un Shapefile.\n",
    "\n",
    "Le shapefile, ou « fichier de formes » est un format de fichier pour les systèmes d'informations géographiques (SIG). Initialement développé par ESRI pour ses logiciels commerciaux, ce format est désormais devenu un standard de facto, dont les spécifications sont ouvertes. Nous allons donc procéder à la lecture d'un exemple de fichier SHP contenant le \"dallage\" global de notre zone à couvrir.\n",
    "\n",
    "Pour cela, nous utilisons la librairie python `pyshp``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[635000.0, 6845000.0, 655000.0, 6875000.0]\n",
      "Record #0: ['./92-2021-0645-6855-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6855000.0), (650000.0, 6855000.0), (650000.0, 6850000.0), (645000.0, 6850000.0), (645000.0, 6855000.0)]\n",
      "Record #1: ['./92-2021-0640-6870-LA93-0M20-E080.jp2']\n",
      "[(640000.0, 6870000.0), (645000.0, 6870000.0), (645000.0, 6865000.0), (640000.0, 6865000.0), (640000.0, 6870000.0)]\n",
      "Record #2: ['./92-2021-0650-6860-LA93-0M20-E080.jp2']\n",
      "[(650000.0, 6860000.0), (655000.0, 6860000.0), (655000.0, 6855000.0), (650000.0, 6855000.0), (650000.0, 6860000.0)]\n",
      "Record #3: ['./92-2021-0645-6850-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6850000.0), (650000.0, 6850000.0), (650000.0, 6845000.0), (645000.0, 6845000.0), (645000.0, 6850000.0)]\n",
      "Record #4: ['./92-2021-0645-6860-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6860000.0), (650000.0, 6860000.0), (650000.0, 6855000.0), (645000.0, 6855000.0), (645000.0, 6860000.0)]\n",
      "Record #5: ['./92-2021-0650-6870-LA93-0M20-E080.jp2']\n",
      "[(650000.0, 6870000.0), (655000.0, 6870000.0), (655000.0, 6865000.0), (650000.0, 6865000.0), (650000.0, 6870000.0)]\n",
      "Record #6: ['./92-2021-0645-6865-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6865000.0), (650000.0, 6865000.0), (650000.0, 6860000.0), (645000.0, 6860000.0), (645000.0, 6865000.0)]\n",
      "Record #7: ['./92-2021-0635-6870-LA93-0M20-E080.jp2']\n",
      "[(635000.0, 6870000.0), (640000.0, 6870000.0), (640000.0, 6865000.0), (635000.0, 6865000.0), (635000.0, 6870000.0)]\n",
      "Record #8: ['./92-2021-0645-6875-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6875000.0), (650000.0, 6875000.0), (650000.0, 6870000.0), (645000.0, 6870000.0), (645000.0, 6875000.0)]\n",
      "Record #9: ['./92-2021-0645-6870-LA93-0M20-E080.jp2']\n",
      "[(645000.0, 6870000.0), (650000.0, 6870000.0), (650000.0, 6865000.0), (645000.0, 6865000.0), (645000.0, 6870000.0)]\n",
      "Record #10: ['./92-2021-0640-6860-LA93-0M20-E080.jp2']\n",
      "[(640000.0, 6860000.0), (645000.0, 6860000.0), (645000.0, 6855000.0), (640000.0, 6855000.0), (640000.0, 6860000.0)]\n",
      "Record #11: ['./92-2021-0650-6875-LA93-0M20-E080.jp2']\n",
      "[(650000.0, 6875000.0), (655000.0, 6875000.0), (655000.0, 6870000.0), (650000.0, 6870000.0), (650000.0, 6875000.0)]\n",
      "Record #12: ['./92-2021-0640-6855-LA93-0M20-E080.jp2']\n",
      "[(640000.0, 6855000.0), (645000.0, 6855000.0), (645000.0, 6850000.0), (640000.0, 6850000.0), (640000.0, 6855000.0)]\n",
      "Record #13: ['./92-2021-0635-6865-LA93-0M20-E080.jp2']\n",
      "[(635000.0, 6865000.0), (640000.0, 6865000.0), (640000.0, 6860000.0), (635000.0, 6860000.0), (635000.0, 6865000.0)]\n",
      "Record #14: ['./92-2021-0650-6855-LA93-0M20-E080.jp2']\n",
      "[(650000.0, 6855000.0), (655000.0, 6855000.0), (655000.0, 6850000.0), (650000.0, 6850000.0), (650000.0, 6855000.0)]\n",
      "Record #15: ['./92-2021-0640-6875-LA93-0M20-E080.jp2']\n",
      "[(640000.0, 6875000.0), (645000.0, 6875000.0), (645000.0, 6870000.0), (640000.0, 6870000.0), (640000.0, 6875000.0)]\n",
      "Record #16: ['./92-2021-0640-6865-LA93-0M20-E080.jp2']\n",
      "[(640000.0, 6865000.0), (645000.0, 6865000.0), (645000.0, 6860000.0), (640000.0, 6860000.0), (640000.0, 6865000.0)]\n",
      "Record #17: ['./92-2021-0635-6860-LA93-0M20-E080.jp2']\n",
      "[(635000.0, 6860000.0), (640000.0, 6860000.0), (640000.0, 6855000.0), (635000.0, 6855000.0), (635000.0, 6860000.0)]\n",
      "Record #18: ['./92-2021-0650-6850-LA93-0M20-E080.jp2']\n",
      "[(650000.0, 6850000.0), (655000.0, 6850000.0), (655000.0, 6845000.0), (650000.0, 6845000.0), (650000.0, 6850000.0)]\n"
     ]
    }
   ],
   "source": [
    "import shapefile\n",
    "\n",
    "url = 's3://poc-automatic-land-parcel-recognition/dataset-reference-fr/092/'\n",
    "\n",
    "myshp = open(url + 'dalles.shp', 'rb', transport_params = {'client': session.client('s3')}) # Shapes\n",
    "mydbf = open(url + 'dalles.dbf', 'rb', transport_params = {'client': session.client('s3')}) # Records\n",
    "myprj = open(url + 'dalles.prj', 'rb', transport_params = {'client': session.client('s3')}) # Projection\n",
    "sr = shapefile.Reader(shp=myshp, dbf=mydbf, prj=myprj)\n",
    "\n",
    "print(sr.bbox) # Bounding Box\n",
    "\n",
    "for n in range(0, sr.numRecords):\n",
    "    print(sr.record(n))\n",
    "    print(sr.shape(n).points)\n",
    "    \n",
    "myshp.close()\n",
    "mydbf.close()\n",
    "myprj.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre but maintenant est de normaliser notre dataset. En entrée, nous avons les Shapefiles contenant les géométries des parcelles et des fichiers images JP2000 ou GeoTIFF :\n",
    "\n",
    "![SampleDataOrthophoto.png](doc/SampleDataOrthophoto.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c114a4653cf10b4ae6402cdcbf2cd3fc44926ff120134afa1c1a380eada19be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
